<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bin Zhu </title> <meta name="author" content="Bin Zhu"> <meta name="description" content=""> <meta name="keywords" content="Multimedia, Multimodal Large Language Model, Egocentric Video Understanding, AI for Healthcare"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://binzhubz.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">service </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Bin Zhu </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?bb9369170263d31fecad467fbb82c6eb" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <a href="https://scholar.google.com.hk/citations?user=xn0ZcJQAAAAJ&amp;hl=en" target="_blank" title="Google Scholar" rel="external nofollow noopener"><i class="ai ai-google-scholar"></i> Google Scholar</a> <p> <a href="mailto:binzhu@smu.edu.sg"><i class="fas fa-envelope"></i> E-mail</a></p> <p> 80 Stamford Road, Singapore 178902 </p> </div> </div> <div class="clearfix"> <p> I am an Assistant Professor of Computer Science at the <a href="https://computing.smu.edu.sg/" rel="external nofollow noopener" target="_blank">School of Computing and Information Systems</a>, <a href="https://www.smu.edu.sg/" rel="external nofollow noopener" target="_blank">Singapore Management University (SMU)</a>. Before joining SMU, I was a Postdoctoral Researcher working with <a href="https://dimadamen.github.io/" rel="external nofollow noopener" target="_blank"> Prof. Dima Damen </a> at the University of Bristol, contributing to the <a href="https://www.robots.ox.ac.uk/~vgg/projects/visualai/" rel="external nofollow noopener" target="_blank">EPSRC Visual AI Program Grant</a> led by <a href="https://www.robots.ox.ac.uk/~az/" rel="external nofollow noopener" target="_blank">Prof. Andrew Zisserman </a>. I earned my Ph.D. degree from <a href="https://www.cs.cityu.edu.hk/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a>, <a href="https://www.cityu.edu.hk" rel="external nofollow noopener" target="_blank">City University of Hong Kong</a> in 2021, under the supervision of <a href="https://faculty.smu.edu.sg/profile/ngo-chong-wah-601" rel="external nofollow noopener" target="_blank">Prof. Chong-Wah Ngo</a> and <a href="https://www.cs.cityu.edu.hk/~wkchan/" rel="external nofollow noopener" target="_blank">Dr. Wing-Kwong Chan</a>. Earlier, I obtained my master and bachelor degrees from <a href="http://www.zju.edu.cn/" rel="external nofollow noopener" target="_blank">Zhejiang University</a> and <a href="http://www.seu.edu.cn/" rel="external nofollow noopener" target="_blank">Southeast University</a> respectively. </p> <p>My research interest lies in Human Centered Multimedia Computing, including Cross-modal retrieval, Multi‚Äêmodal Large Language Model, Egocentric Video Understanding, Generative models and AI for Healthcare. Specifically, the objective is to conduct frontier research and develop cutting-edge technologies for processing, modeling, analyzing, understanding and generating multimedia content, that facilitate natural and immersive human experience and exert positive impact for our society. </p> <p>üî•üî•üî•<span style="color: rgb(255, 0, 0)">Openings:</span> I am actively looking for self-motivated Ph.D. students, CSC visiting students, and (remote) interns. In addition, I am looking for postdoctoral researchers and funded visiting students to work on embodied AI. If you are interested in working with me, please feel free to drop me an email with your CV and other supporting documents (if any). </p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 08, 2025</th> <td> I was invited to deliver a talk at the <a href="https://www.nextcenter.org/" rel="external nofollow noopener" target="_blank">National University of Singapore NExT Research Centre</a> on ‚ÄúFood Computing from an Egocentric Video Perspective‚Äù. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 02, 2025</th> <td> I was honored to serve on the Board of Examiners for Ph.D. Candidate <a href="https://gabrielegoletto.github.io/" rel="external nofollow noopener" target="_blank">Gabriele Goletto</a>, whose thesis focused on Egocentric Vision. Congratulations to Dr. Goletto on a successful defense! üéì </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 22, 2025</th> <td> I am awarded a <a href="https://www.moe.gov.sg/" rel="external nofollow noopener" target="_blank">Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 2 grant</a> as Principal Investigator for my project ‚ÄúSelf-Adaptive Planning with Environmental Awareness for Embodied Agents‚Äù, with total funding of SGD$959,166. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 19, 2025</th> <td> One paper on Large Lithium-ion Battery Model is accepted by <a href="https://www.nature.com/ncomms" rel="external nofollow noopener" target="_blank">Nature Communications</a> as co-corresponding author. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 05, 2025</th> <td> One paper on Multimodal Large Language Model is accepted by ACM MM 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 26, 2025</th> <td> One paper on Multimodal Large Language Model is accepted by ICCV 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 19, 2025</th> <td> One paper on Recipe Progress Tracking in Non-Visual Cooking are accepted by ASSETS 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 09, 2025</th> <td> One paper on Cooking Procedural Image Generation is accepted by ACM TOMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 06, 2025</th> <td> I will serve as the <a href="https://sites.google.com/view/www-acmicmr-org" rel="external nofollow noopener" target="_blank">Program Co-Chair for ACM ICMR 2027</a>, which will be held in Singapore! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 23, 2025</th> <td> One paper on Nutrition Estimation is accepted by ICMR 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 21, 2025</th> <td> One paper on Ingredient Recognition is accepted by ICME 2025 (oral). </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 08, 2025</th> <td> One paper on Egocentric Video Understanding is accepted by CVPR 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 23, 2025</th> <td> One paper on Recipe Following in Cooking Video is accepted by CHI (LBW) 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 16, 2025</th> <td> One paper on Large Multimodal Model in Food Domain is accepted by IEEE TMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 09, 2024</th> <td> Two papers on Grasp Generation and Text-to-Hand-Image Generation are accepted by AAAI 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 05, 2024</th> <td> We are excited to announce our <a href="https://2025.ieeeicme.org/ss10-multimedia-for-cooking-and-eating-activities/" rel="external nofollow noopener" target="_blank">Special Session on Multimedia for Cooking and Eating Activities </a> at ICME 2025. We warmly invite you to submit your papers! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 29, 2024</th> <td> One paper on Recipe Generation is accepted by WACV 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2024</th> <td> One paper on Cross-modal Recipe Retrieval is accepted by ECCV 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2024</th> <td> One paper on Time-series Weight Prediction is accepted by ACM MM 2024 and is further selected as an Oral presentation (3.97%). </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 15, 2024</th> <td> One paper on Text-driven Video Prediction is accepted by ACM TOMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 15, 2024</th> <td> Two papers on Unsupervised Video Hashing and Generalizable Food Recognition are accepted by IEEE TMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 01, 2024</th> <td> I joined <a href="https://www.smu.edu.sg/" rel="external nofollow noopener" target="_blank"> Singapore Management University </a> as an Assistant Professor of Computer Science. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NC</abbr> <figure> <picture> <img src="/assets/img/publication_preview/NC2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="NC2025.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="LLiM" class="col-sm-8"> <div class="title">LLiM: Large Lithium-ion Battery Model for Secure Shared E-bike Battery in Smart Cities</div> <div class="author"> Donghui Ding, Zhao Li, Linhao Luo, Ming Jin, <em>Bin Zhu</em>, Yichen Zhong, Junhao Hu, Peng Cai, and Huiqi Hu </div> <div class="periodical"> <em>Nature Communications</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="#" class="btn btn-sm z-depth-0" role="button">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/ICCV25-Dual-LoRA.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ICCV25-Dual-LoRA.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dual-LoRA" class="col-sm-8"> <div class="title">From Holistic to Localized: Local Enhanced Adapters for Efficient Visual Instruction Fine-Tuning</div> <div class="author"> Pengkun Jiao, <em>Bin Zhu</em>, Jingjing Chen, Chong-Wah Ngo, and Yugang Jiang </div> <div class="periodical"> <em>In International Conference on Computer Vision</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2411.12787" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/pengkun-jiao/Dual-LoRA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/MM2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="MM2025.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2024look" class="col-sm-8"> <div class="title">Look before you decide: Prompting active deduction of mllms for assumptive reasoning</div> <div class="author"> Yian Li, Wentao Tian, Yang Jiao, Jingjing Chen, Tianwen Qian, <em>Bin Zhu</em>, Na Zhao, and Yu-Gang Jiang </div> <div class="periodical"> <em>In Proceedings of the 33rd ACM International Conference on Multimedia</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2404.12966" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="#" class="btn btn-sm z-depth-0" role="button">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/HDEPIC.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="HDEPIC.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="HDEPIC" class="col-sm-8"> <div class="title">HD-EPIC: A highly-detailed egocentric video dataset</div> <div class="author"> Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Kumar Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, Jacob Chalk, Zhifan Zhu, Rhodri Guerrier, Fahd Abdelazim, <em>Bin Zhu</em>, Davide Moltisanti, Michael Wray, Hazel Doughty, and Dima Damen </div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Perrett_HD-EPIC_A_Highly-Detailed_Egocentric_Video_Dataset_CVPR_2025_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://hd-epic.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Hand1000.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Hand1000.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025hand1000" class="col-sm-8"> <div class="title">Hand1000: Generating realistic hands from text with only 1,000 images</div> <div class="author"> Haozhuo Zhang, <em>Bin Zhu</em>, Yu Cao, and Yanbin Hao </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/33074" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Haozhuo-Zhang/Hand1000" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://haozhuo-zhang.github.io/Hand1000-project-page/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/HandGrasp.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="HandGrasp.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025ragg" class="col-sm-8"> <div class="title">RAGG: Retrieval-Augmented Grasp Generation Model</div> <div class="author"> Zhenhua Tang, <em>Bin Zhu</em>, Yanbin Hao, Chong-Wah Ngo, and Richang Hong </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32786" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TMM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/FoodLMM.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FoodLMM.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yin2023foodlmm" class="col-sm-8"> <div class="title">FoodLMM: A versatile food assistant using large multi-modal model</div> <div class="author"> Yuehao Yin, Huiyan Qi, <em>Bin Zhu</em>, Jingjing Chen, Yu-Gang Jiang, and Chong-Wah Ngo </div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2312.14991" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/YuehaoYin/FoodLMM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ArXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/GaslightingBench.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GaslightingBench.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2025calling" class="col-sm-8"> <div class="title">Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation</div> <div class="author"> <em>Bin Zhu</em>, Huiyan Qi, Yinxuan Gui, Jingjing Chen, Chong-Wah Ngo, and Ee-Peng Lim </div> <div class="periodical"> <em>arXiv preprint arXiv:2501.19017</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2501.19017" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ArXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/GaslightingBench-R.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GaslightingBench-R.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2025reasoning" class="col-sm-8"> <div class="title">Reasoning Models Are More Easily Gaslighted Than You Think</div> <div class="author"> <em>Bin Zhu</em>, Hailong Yin, Jingjing Chen, and Yu-Gang Jiang </div> <div class="periodical"> <em>arXiv preprint arXiv:2506.09677</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2506.09677" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://binzhubz.github.io/GaslightingBench-R/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ArXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Gaslighting-Attention.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Gaslighting-Attention.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiao2025don" class="col-sm-8"> <div class="title">Don‚Äôt Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs</div> <div class="author"> Pengkun Jiao, <em>Bin Zhu</em>, Jingjing Chen, Chong-Wah Ngo, and Yu-Gang Jiang </div> <div class="periodical"> <em>arXiv preprint arXiv:2504.09456</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2504.09456" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MM Oral</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_weightprediction.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_weightprediction.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gui2024navigating" class="col-sm-8"> <div class="title">Navigating weight prediction with diet diary</div> <div class="author"> Yinxuan Gui, <em>Bin Zhu</em>, Jingjing Chen, Chong Wah Ngo, and Yu-Gang Jiang </div> <div class="periodical"> <em>In Proceedings of the 32nd ACM International Conference on Multimedia</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2408.05445" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://yxg1005.github.io/weight-prediction/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_DAR.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_DAR.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="song2024enhancing" class="col-sm-8"> <div class="title">Enhancing recipe retrieval with foundation models: A data augmentation perspective</div> <div class="author"> Fangzhou Song, <em>Bin Zhu</em>, Yanbin Hao, and Shuo Wang </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06751.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Noah888/DAR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_CgT-GAN.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_CgT-GAN.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2023cgt" class="col-sm-8"> <div class="title">CgT-GAN: clip-guided text GAN for image captioning</div> <div class="author"> Jiarui Yu, Haoran Li, Yanbin Hao, <em>Bin Zhu</em>, Tong Xu, and Xiangnan He </div> <div class="periodical"> <em>In Proceedings of the 31st ACM International Conference on Multimedia</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2308.12045" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Lihr747/CgtGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_VISOR.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_VISOR.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="darkhalil2022epic" class="col-sm-8"> <div class="title">Epic-kitchens visor benchmark: Video segmentations and object relations</div> <div class="author"> Ahmad Darkhalil, Dandan Shan, <em>Bin Zhu</em>, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/590a7ebe0da1f262c80d0188f5c4c222-Paper-Datasets_and_Benchmarks.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://epic-kitchens.github.io/VISOR/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TIP</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_VIREOFood251.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_VIREOFood251.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2020study" class="col-sm-8"> <div class="title">A study of multi-task and region-wise deep learning for food ingredient recognition</div> <div class="author"> Jingjing Chen, <em>Bin Zhu</em>, Chong-Wah Ngo, Tat-Seng Chua, and Yu-Gang Jiang </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7304&amp;context=sis_research" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_crossdomain.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_crossdomain.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2020cross" class="col-sm-8"> <div class="title">Cross-domain cross-modal food transfer</div> <div class="author"> <em>Bin Zhu</em>, Chong-Wah Ngo, and Jing-jing Chen </div> <div class="periodical"> <em>In Proceedings of the 28th ACM International Conference on Multimedia</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7500&amp;context=sis_research" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_CookGAN.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_CookGAN.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2020cookgan" class="col-sm-8"> <div class="title">CookGAN: Causality based text-to-image synthesis</div> <div class="author"> <em>Bin Zhu</em> and Chong-Wah Ngo </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_CookGAN_Causality_Based_Text-to-Image_Synthesis_CVPR_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_R2GAN.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_R2GAN.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2019r2gan" class="col-sm-8"> <div class="title">R2GAN: Cross-modal recipe retrieval with generative adversarial network</div> <div class="author"> <em>Bin Zhu</em>, Chong-Wah Ngo, Jingjing Chen, and Yanbin Hao </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_R2GAN_Cross-Modal_Recipe_Retrieval_With_Generative_Adversarial_Network_CVPR_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Bin Zhu. @Copyright 2025 Bin Zhu. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>