<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bin Zhu </title> <meta name="author" content="Bin Zhu"> <meta name="description" content=""> <meta name="keywords" content="Multimedia, Multimodal Large Language Model, Egocentric Video Understanding, AI for Healthcare"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://binzhubz.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">service </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Bin Zhu </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?bb9369170263d31fecad467fbb82c6eb" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <a href="https://scholar.google.com.hk/citations?user=xn0ZcJQAAAAJ&amp;hl=en" target="_blank" title="Google Scholar" rel="external nofollow noopener"><i class="ai ai-google-scholar"></i> Google Scholar</a> <p> <a href="mailto:binzhu@smu.edu.sg"><i class="fas fa-envelope"></i> E-mail</a></p> <p> 80 Stamford Road, Singapore 178902 </p> </div> </div> <div class="clearfix"> <p> I am an Assistant Professor of Computer Science at the <a href="https://computing.smu.edu.sg/" rel="external nofollow noopener" target="_blank">School of Computing and Information Systems</a>, <a href="https://www.smu.edu.sg/" rel="external nofollow noopener" target="_blank">Singapore Management University (SMU)</a>. Before joining SMU, I was a Postdoctoral Researcher working with <a href="https://dimadamen.github.io/" rel="external nofollow noopener" target="_blank"> Prof. Dima Damen </a> at the University of Bristol, contributing to the <a href="https://www.robots.ox.ac.uk/~vgg/projects/visualai/" rel="external nofollow noopener" target="_blank">EPSRC Visual AI Program Grant</a> led by <a href="https://www.robots.ox.ac.uk/~az/" rel="external nofollow noopener" target="_blank">Prof. Andrew Zisserman </a>. I earned my Ph.D. degree from <a href="https://www.cs.cityu.edu.hk/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a>, <a href="https://www.cityu.edu.hk" rel="external nofollow noopener" target="_blank">City University of Hong Kong</a> in 2021, under the supervision of <a href="https://faculty.smu.edu.sg/profile/ngo-chong-wah-601" rel="external nofollow noopener" target="_blank">Prof. Chong-Wah Ngo</a> and <a href="https://www.cs.cityu.edu.hk/~wkchan/" rel="external nofollow noopener" target="_blank">Dr. Wing-Kwong Chan</a>. Earlier, I obtained my master and bachelor degrees from <a href="http://www.zju.edu.cn/" rel="external nofollow noopener" target="_blank">Zhejiang University</a> and <a href="http://www.seu.edu.cn/" rel="external nofollow noopener" target="_blank">Southeast University</a> respectively. </p> <p>My research interest lies in Human Centered Multimedia Computing, including Cross-modal retrieval, Multi‚Äêmodal Large Language Model, Egocentric Video Understanding, Generative models and AI for Healthcare. Specifically, the objective is to conduct frontier research and develop cutting-edge technologies for processing, modeling, analyzing, understanding and generating multimedia content, that facilitate natural and immersive human experience and exert positive impact for our society. </p> <p>üî•üî•üî•<span style="color: rgb(255, 0, 0)">Openings:</span> I am actively looking for self-motivated Ph.D. students, CSC visiting students, and (remote) interns. If you are interested in working with me, please feel free to drop me an email with your CV and other supporting documents (if any). </p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 09, 2025</th> <td> One paper on Cooking Procedural Image Generation is accepted by ACM TOMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 06, 2025</th> <td> I will serve as the Program Co-Chair for ACM ICMR 2027, which will be held in Singapore! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 23, 2025</th> <td> One paper on Nutrition Estimation is accepted by ICMR 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 21, 2025</th> <td> One paper on Ingredient Recognition is accepted by ICME 2025 (oral). </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 08, 2025</th> <td> One paper on Egocentric Video Understanding is accepted by CVPR 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 23, 2025</th> <td> One paper on Recipe Following in Cooking Video is accepted by CHI (LBW) 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 16, 2025</th> <td> One paper on Large Multimodal Model in Food Domain is accepted by IEEE TMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 09, 2024</th> <td> Two papers on Grasp Generation and Text-to-Hand-Image Generation are accepted by AAAI 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 05, 2024</th> <td> We are excited to announce our <a href="https://2025.ieeeicme.org/ss10-multimedia-for-cooking-and-eating-activities/" rel="external nofollow noopener" target="_blank">Special Session on Multimedia for Cooking and Eating Activities </a> at ICME 2025. We warmly invite you to submit your papers! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 29, 2024</th> <td> One paper on Recipe Generation is accepted by WACV 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2024</th> <td> One paper on Cross-modal Recipe Retrieval is accepted by ECCV 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2024</th> <td> One paper on Time-series Weight Prediction is accepted by ACM MM 2024 and is further selected as an Oral presentation (3.97%). </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 15, 2024</th> <td> One paper on Text-driven Video Prediction is accepted by ACM TOMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 15, 2024</th> <td> Two papers on Unsupervised Video Hashing and Generalizable Food Recognition are accepted by IEEE TMM. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 01, 2024</th> <td> I joined <a href="https://www.smu.edu.sg/" rel="external nofollow noopener" target="_blank"> Singapore Management University </a> as an Assistant Professor of Computer Science. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/HDEPIC.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="HDEPIC.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="HDEPIC" class="col-sm-8"> <div class="title">HD-EPIC: A highly-detailed egocentric video dataset</div> <div class="author"> Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Kumar Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, Jacob Chalk, Zhifan Zhu, Rhodri Guerrier, Fahd Abdelazim, Bin Zhu, Davide Moltisanti, Michael Wray, Hazel Doughty, and Dima Damen </div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Perrett_HD-EPIC_A_Highly-Detailed_Egocentric_Video_Dataset_CVPR_2025_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://hd-epic.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Hand1000.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Hand1000.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025hand1000" class="col-sm-8"> <div class="title">Hand1000: Generating realistic hands from text with only 1,000 images</div> <div class="author"> Haozhuo Zhang, Bin Zhu, Yu Cao, and Yanbin Hao </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/33074" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Haozhuo-Zhang/Hand1000" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://haozhuo-zhang.github.io/Hand1000-project-page/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/HandGrasp.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="HandGrasp.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025ragg" class="col-sm-8"> <div class="title">RAGG: Retrieval-Augmented Grasp Generation Model</div> <div class="author"> Zhenhua Tang, Bin Zhu, Yanbin Hao, Chong-Wah Ngo, and Richang Hong </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32786" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TMM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/FoodLMM.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FoodLMM.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yin2023foodlmm" class="col-sm-8"> <div class="title">Foodlmm: A versatile food assistant using large multi-modal model</div> <div class="author"> Yuehao Yin, Huiyan Qi, Bin Zhu, Jingjing Chen, Yu-Gang Jiang, and Chong-Wah Ngo </div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2312.14991" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/YuehaoYin/FoodLMM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ArXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/GaslightingBench.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GaslightingBench.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2025calling" class="col-sm-8"> <div class="title">Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation</div> <div class="author"> Bin Zhu, Huiyan Qi, Yinxuan Gui, Jingjing Chen, Chong-Wah Ngo, and Ee-Peng Lim </div> <div class="periodical"> <em>arXiv preprint arXiv:2501.19017</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2501.19017" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MM Oral</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_weightprediction.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_weightprediction.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gui2024navigating" class="col-sm-8"> <div class="title">Navigating weight prediction with diet diary</div> <div class="author"> Yinxuan Gui, Bin Zhu, Jingjing Chen, Chong Wah Ngo, and Yu-Gang Jiang </div> <div class="periodical"> <em>In Proceedings of the 32nd ACM International Conference on Multimedia</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2408.05445" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://yxg1005.github.io/weight-prediction/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cover_DAR.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cover_DAR.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="song2024enhancing" class="col-sm-8"> <div class="title">Enhancing recipe retrieval with foundation models: A data augmentation perspective</div> <div class="author"> Fangzhou Song, Bin Zhu, Yanbin Hao, and Shuo Wang </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06751.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Noah888/DAR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://inspirehep.net/authors/1010907" title="Inspire HEP" rel="external nofollow noopener" target="_blank"><i class="ai ai-inspire"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=qc6CJjYAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.alberteinstein.com/" title="Custom Social" rel="external nofollow noopener" target="_blank"> <img src="https://www.alberteinstein.com/wp-content/uploads/2024/03/cropped-favicon-192x192.png" alt="Custom Social"> </a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Bin Zhu. @Copyright 2025 Bin Zhu. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>